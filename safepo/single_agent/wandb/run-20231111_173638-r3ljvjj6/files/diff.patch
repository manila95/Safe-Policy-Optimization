diff --git a/safepo/single_agent/trpo_lag.py b/safepo/single_agent/trpo_lag.py
index f2bdffd..59f1340 100644
--- a/safepo/single_agent/trpo_lag.py
+++ b/safepo/single_agent/trpo_lag.py
@@ -42,6 +42,9 @@ from safepo.common.lagrange import Lagrange
 from safepo.common.logger import EpochLogger
 from safepo.common.model import ActorVCritic
 from safepo.utils.config import single_agent_args, isaac_gym_map, parse_sim_params
+from src.models.risk_models import *
+from src.datasets.risk_datasets import *
+from src.utils import * 
 
 CONJUGATE_GRADIENT_ITERS=15
 TRPO_SEARCHING_STEPS=15
@@ -175,6 +178,7 @@ def main(args, cfg_env=None):
                 monitor_gym=True,
                 sync_tensorboard=True, save_code=True)
 
+    risk_size = args.quantile_num if args.risk_type == "quantile" else 2
 
     if args.task not in isaac_gym_map.keys():
         env, obs_space, act_space = make_sa_mujoco_env(
@@ -210,6 +214,29 @@ def main(args, cfg_env=None):
         policy.cost_critic.parameters(), lr=1e-3
     )
 
+    if args.use_risk:
+        risk_model_class = {"bayesian": {"continuous": BayesRiskEstCont, "binary": BayesRiskEst, "quantile": BayesRiskEst}, 
+                    "mlp": {"continuous": RiskEst, "binary": RiskEst}} 
+
+        risk_model = BayesRiskEst(obs_size=obs_space.shape[0], batch_norm=True, out_size=risk_size)
+        if os.path.exists(args.risk_model_path):
+            risk_model.load_state_dict(torch.load(args.risk_model_path, map_location=device))
+
+        risk_model.to(device)
+        risk_model.eval()
+
+        opt_risk = torch.optim.Adam(risk_model.parameters(), lr=args.risk_lr, eps=1e-10)
+
+        if args.fine_tune_risk:
+            rb = ReplayBuffer(buffer_size=args.total_steps)
+
+            if args.risk_type == "quantile":
+                weight_tensor = torch.Tensor([1]*args.quantile_num).to(device)
+                weight_tensor[0] = args.risk_weight
+            elif args.risk_type == "binary":
+                weight_tensor = torch.Tensor([1., args.risk_weight]).to(device)
+            risk_criterion = nn.NLLLoss(weight=weight_tensor)
+
     # create the vectorized on-policy buffer
     buffer = VectorizedOnPolicyBuffer(
         obs_space=obs_space,
@@ -250,6 +277,7 @@ def main(args, cfg_env=None):
         np.zeros(args.num_envs),
     )
     total_cost, eval_total_cost = 0, 0
+    f_next_obs, f_costs = None, None
 
     # training loop
     for epoch in range(epochs):
@@ -257,7 +285,11 @@ def main(args, cfg_env=None):
         # collect samples until we have enough to update
         for steps in range(local_steps_per_epoch):
             with torch.no_grad():
-                act, log_prob, value_r, value_c = policy.step(obs, deterministic=False)
+                if args.use_risk:
+                    risk = risk_model(obs)
+                    act, log_prob, value_r, value_c = policy.step(obs, risk, deterministic=False)
+                else:
+                    act, log_prob, value_r, value_c = policy.step(obs, deterministic=False)
             action = act.detach().squeeze() if args.task in isaac_gym_map.keys() else act.detach().squeeze().cpu().numpy()
             next_obs, reward, cost, terminated, truncated, info = env.step(action)
 
@@ -268,6 +300,10 @@ def main(args, cfg_env=None):
                 torch.as_tensor(x, dtype=torch.float32, device=device)
                 for x in (next_obs, reward, cost, terminated, truncated)
             )
+            if args.use_risk and args.fine_tune_risk:
+                f_next_obs = next_obs.unsqueeze(0) if f_next_obs is None else torch.concat([f_next_obs, next_obs.unsqueeze(0)], axis=0)
+                f_costs = cost.unsqueeze(0) if f_costs is None else torch.concat([f_costs, cost.unsqueeze(0)], axis=0)
+            # print(info)
             if "final_observation" in info:
                 info["final_observation"] = np.array(
                     [
@@ -280,6 +316,15 @@ def main(args, cfg_env=None):
                     dtype=torch.float32,
                     device=device,
                 )
+                if args.use_risk and args.fine_tune_risk:
+                    f_risks = torch.empty_like(f_costs)
+                    for i in range(args.num_envs):
+                        f_risks[:, i] = compute_fear(f_costs[:, i])
+                    rb.add(None, f_next_obs.view(-1, obs_space.shape[0]), None, None, None, None, f_risks.view(-1, 1), f_risks.view(-1, 1))
+
+                    f_next_obs, f_costs = None, None
+
+                final_risk = risk_model(info["final_observation"]) if args.use_risk else None
             buffer.store(
                 obs=obs,
                 act=act,
@@ -291,6 +336,8 @@ def main(args, cfg_env=None):
             )
 
             obs = next_obs
+            risk = risk_model(obs) if args.use_risk else None
+
             epoch_end = steps >= local_steps_per_epoch - 1
             for idx, (done, time_out) in enumerate(zip(terminated, truncated)):
                 if epoch_end or done or time_out:
@@ -299,14 +346,24 @@ def main(args, cfg_env=None):
                     if not done:
                         if epoch_end:
                             with torch.no_grad():
-                                _, _, last_value_r, last_value_c = policy.step(
-                                    obs[idx], deterministic=False
-                                )
+                                if args.use_risk:
+                                    _, _, last_value_r, last_value_c = policy.step(
+                                        obs[idx], risk[idx], deterministic=False
+                                    )
+                                else:
+                                    _, _, last_value_r, last_value_c = policy.step(
+                                        obs[idx], deterministic=False
+                                    )
                         if time_out:
                             with torch.no_grad():
-                                _, _, last_value_r, last_value_c = policy.step(
-                                    info["final_observation"][idx], deterministic=False
-                                )
+                                if args.use_risk:
+                                    _, _, last_value_r, last_value_c = policy.step(
+                                        info["final_observation"][idx], final_risk[idx], deterministic=False
+                                    )
+                                else:  
+                                    _, _, last_value_r, last_value_c = policy.step(
+                                        info["final_observation"][idx], deterministic=False
+                                    )
                         last_value_r = last_value_r.unsqueeze(0)
                         last_value_c = last_value_c.unsqueeze(0)
                     if done or time_out:
@@ -343,7 +400,11 @@ def main(args, cfg_env=None):
                 eval_rew, eval_cost, eval_len = 0.0, 0.0, 0.0
                 while not eval_done:
                     with torch.no_grad():
-                        act, log_prob, value_r, value_c = policy.step(eval_obs, deterministic=True)
+                        if args.use_risk:
+                            risk = risk_model(eval_obs)
+                            act, log_prob, value_r, value_c = policy.step(eval_obs, risk, deterministic=True)
+                        else:
+                            act, log_prob, value_r, value_c = policy.step(eval_obs, deterministic=True)
                     next_obs, reward, cost, terminated, truncated, info = env.step(
                         act.detach().squeeze().cpu().numpy()
                     )
@@ -407,6 +468,17 @@ def main(args, cfg_env=None):
 
         final_kl = 0.0
 
+        ## Risk Fine Tuning before the policy is updated
+        if args.use_risk and args.fine_tune_risk:
+            risk_data = rb.sample(args.num_risk_samples)
+            risk_dataset = RiskyDataset(risk_data["next_obs"].to('cpu'), None, risk_data["risks"].to('cpu'), False, risk_type=args.risk_type,
+                                    fear_clip=None, fear_radius=args.fear_radius, one_hot=True, quantile_size=args.quantile_size, quantile_num=args.quantile_num)
+            risk_dataloader = DataLoader(risk_dataset, batch_size=args.risk_batch_size, shuffle=True)
+
+            risk_loss = train_risk(risk_model, risk_dataloader, risk_criterion, opt_risk, args.num_risk_epochs, device)
+            logger.store(*{"risk/risk_loss": risk_loss})
+
+
         # While not within_trust_region and not out of total_steps:
         for step in range(TRPO_SEARCHING_STEPS):
             # update theta params
@@ -415,12 +487,20 @@ def main(args, cfg_env=None):
             set_param_values_to_model(policy.actor, new_theta)
 
             with torch.no_grad():
-                temp_distribution = policy.actor(data["obs"])
+                if args.use_risk:
+                    risk = risk_model(data["obs"])
+                    temp_distribution = policy.actor(data["obs"], risk)
+                else:
+                    temp_distribution = policy.actor(data["obs"])
                 log_prob = temp_distribution.log_prob(data["act"]).sum(dim=-1)
                 ratio = torch.exp(log_prob - data["log_prob"])
                 loss_pi = -(ratio * advantage).mean()
                 # compute KL distance between new and old policy
-                current_distribution = policy.actor(data["obs"])
+                if args.use_risk:
+                    risk = risk_model(data["obs"])
+                    current_distribution = policy.actor(data["obs"], risk)
+                else:
+                    current_distribution = policy.actor(data["obs"])
                 kl = (
                     torch.distributions.kl.kl_divergence(
                         old_distribution, current_distribution
@@ -482,10 +562,11 @@ def main(args, cfg_env=None):
                 target_value_r_b,
                 target_value_c_b,
             ) in dataloader:
+                risk_b = risk_model(obs_b) if args.use_risk else None
                 reward_critic_optimizer.zero_grad()
-                loss_r = nn.functional.mse_loss(policy.reward_critic(obs_b), target_value_r_b)
+                loss_r = nn.functional.mse_loss(policy.reward_critic(obs_b, risk_b), target_value_r_b)
                 cost_critic_optimizer.zero_grad()
-                loss_c = nn.functional.mse_loss(policy.cost_critic(obs_b), target_value_c_b)
+                loss_c = nn.functional.mse_loss(policy.cost_critic(obs_b, risk_b), target_value_c_b)
                 if config.get("use_critic_norm", True):
                     for param in policy.reward_critic.parameters():
                         loss_r += param.pow(2).sum() * 0.001
@@ -538,7 +619,8 @@ def main(args, cfg_env=None):
             logger.log_tabular("Misc/gradient_norm")
             logger.log_tabular("Misc/H_inv_g")
             logger.log_tabular("Misc/AcceptanceStep")
-
+            if args.use_risk and args.fine_tune_risk:
+                logger.log_tabular("Risk/Risk Loss", risk_loss)
             logger.dump_tabular()
             if (epoch+1) % 100 == 0 or epoch == 0:
                 logger.torch_save(itr=epoch)
